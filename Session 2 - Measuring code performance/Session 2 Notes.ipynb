{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 2: Analyzing Code Performance\n",
    "\n",
    "#### Topics.\n",
    " 1. Timing Julia functions\n",
    " 2. Accurate benchmarking\n",
    " 3. Profiling Julia functions\n",
    " 4. (optional) Tracking detailed memory allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m new environment at `~/Documents/Github2/Physics-215-Julia/Session 2 - Measuring code performance/Project.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Session 2: Measuring code performance**\n",
    "\n",
    "- Objective: Submit a performance analysis of a self-implemented physics textbook function or constant using Julia benchmarking tools.\n",
    "- KR1: Implemented (customized) at least one math/physics textbook function, or constant (prefer those that involve a sum or a loop) in Julia. Discuss its importance in Physics. See Resources below.\n",
    "- KR2: Compare the performance (accuracy) of the implemented function in comparison with the existing special functions within Julia.\n",
    "- KR3: Successful loading of the `BenchmarkTools` module. May need to add it first via the `Pkg` or REPL package mode.\n",
    "- KR4: Itemized differences between `@time`, `@btime`, `@benchmark` and other `@time`-like macros. Nice if the situations when they are best applied are mentioned.\n",
    "- KR5:  Identified demonstrated useful features within the `Profiler` module of Julia. Features must be explained why useful for your case.\n",
    "- KR6: A discussion of the performance of the implemented function above.\n",
    "- KR7: Disuss the computational complexity of the Madelbrot function you made based on KR5. What is the best `@time` output to use for this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seql_sum"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    seql_sum(x)\n",
    "\n",
    "A naive cumulative function.\n",
    "Input: iterable container `x`\n",
    "Output: sum of all elements of `x`\n",
    "\"\"\"\n",
    "function seql_sum(x)\n",
    "    result = zero(eltype(x))\n",
    "    for i in eachindex(x)\n",
    "        result += x[i]\n",
    "    end\n",
    "    return result\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10_000_000\n",
    "x64 = rand(N) #Float64 type\n",
    "x32 = rand(Float32, 2*length(x64)); #same memory length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NOTES:\n",
      "> eps(eltype(x64)) is 2.2204e-16.\n",
      "> Statistical tolerance is  0.00031622776601683794.\n"
     ]
    }
   ],
   "source": [
    "println(\"\\nNOTES:\")\n",
    "println(\"> eps(eltype(x64)) is $(round(eps(eltype(x64)), digits = 20)).\");\n",
    "total = 1/sqrt(length(x64))\n",
    "println(\"> Statistical tolerance is  $(round(total, digits = 20)).\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NamedTuple{(:value, :time, :bytes, :gctime, :gcstats), Tuple{Float64, Float64, Int64, Float64, Base.GC_Diff}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Timing seql_sum()\n",
    "\n",
    "base64 = @timed sum(x64);\n",
    "base64 = @timed sum(x64);\n",
    "\n",
    "base32 = @timed sum(x32);\n",
    "base32 = @timed sum(x32);\n",
    "\n",
    "typeof(base64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time t64 for sum(x64) is 0.005808245.\n",
      "The time t32 for sum(x32) is 0.005970829.\n",
      "     such that t32/t64 = 1.02799.\n",
      "               t64/t64 = 1.0.\n",
      "     and       t32/t32 = 1.0.\n"
     ]
    }
   ],
   "source": [
    "t64 = base64[:time]\n",
    "t32 = base32[:time]\n",
    "\n",
    "println(\"The time t64 for sum(x64) is $(t64).\")\n",
    "println(\"The time t32 for sum(x32) is $(t32).\")\n",
    "println(\"     such that t32/t64 = $(round((t32/t64), digits = 5)).\")\n",
    "println(\"               t64/t64 = $(round((t64/t64), digits = 5)).\")\n",
    "println(\"     and       t32/t32 = $(round((t32/t32), digits = 5)).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "@btime expression [other parameters...]\n",
       "\\end{verbatim}\n",
       "Similar to the \\texttt{@time} macro included with Julia, this executes an expression, printing the time it took to execute and the memory allocated before returning the value of the expression.\n",
       "\n",
       "Unlike \\texttt{@time}, it uses the \\texttt{@benchmark} macro, and accepts all of the same additional parameters as \\texttt{@benchmark}.  The printed time is the \\emph{minimum} elapsed time measured during the benchmark.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "@btime expression [other parameters...]\n",
       "```\n",
       "\n",
       "Similar to the `@time` macro included with Julia, this executes an expression, printing the time it took to execute and the memory allocated before returning the value of the expression.\n",
       "\n",
       "Unlike `@time`, it uses the `@benchmark` macro, and accepts all of the same additional parameters as `@benchmark`.  The printed time is the *minimum* elapsed time measured during the benchmark.\n"
      ],
      "text/plain": [
       "\u001b[36m  @btime expression [other parameters...]\u001b[39m\n",
       "\n",
       "  Similar to the \u001b[36m@time\u001b[39m macro included with Julia, this executes an expression,\n",
       "  printing the time it took to execute and the memory allocated before\n",
       "  returning the value of the expression.\n",
       "\n",
       "  Unlike \u001b[36m@time\u001b[39m, it uses the \u001b[36m@benchmark\u001b[39m macro, and accepts all of the same\n",
       "  additional parameters as \u001b[36m@benchmark\u001b[39m. The printed time is the \u001b[4mminimum\u001b[24m elapsed\n",
       "  time measured during the benchmark."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?@btime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  11.790 ms (0 allocations: 0 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.000127522141715e6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@btime seql_sum($x64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "@benchmark <expr to benchmark> [setup=<setup expr>]\n",
       "\\end{verbatim}\n",
       "Run benchmark on a given expression.\n",
       "\n",
       "\\section{Example}\n",
       "The simplest usage of this macro is to put it in front of what you want to benchmark.\n",
       "\n",
       "\\begin{verbatim}\n",
       "julia> @benchmark sin(1)\n",
       "BenchmarkTools.Trial:\n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     13.610 ns (0.00% GC)\n",
       "  median time:      13.622 ns (0.00% GC)\n",
       "  mean time:        13.638 ns (0.00% GC)\n",
       "  maximum time:     21.084 ns (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     998\n",
       "\\end{verbatim}\n",
       "You can interpolate values into \\texttt{@benchmark} expressions:\n",
       "\n",
       "\\begin{verbatim}\n",
       "# rand(1000) is executed for each evaluation\n",
       "julia> @benchmark sum(rand(1000))\n",
       "BenchmarkTools.Trial:\n",
       "  memory estimate:  7.94 KiB\n",
       "  allocs estimate:  1\n",
       "  --------------\n",
       "  minimum time:     1.566 μs (0.00% GC)\n",
       "  median time:      2.135 μs (0.00% GC)\n",
       "  mean time:        3.071 μs (25.06% GC)\n",
       "  maximum time:     296.818 μs (95.91% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     10\n",
       "\n",
       "# rand(1000) is evaluated at definition time, and the resulting\n",
       "# value is interpolated into the benchmark expression\n",
       "julia> @benchmark sum($(rand(1000)))\n",
       "BenchmarkTools.Trial:\n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     101.627 ns (0.00% GC)\n",
       "  median time:      101.909 ns (0.00% GC)\n",
       "  mean time:        103.834 ns (0.00% GC)\n",
       "  maximum time:     276.033 ns (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     935\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "@benchmark <expr to benchmark> [setup=<setup expr>]\n",
       "```\n",
       "\n",
       "Run benchmark on a given expression.\n",
       "\n",
       "# Example\n",
       "\n",
       "The simplest usage of this macro is to put it in front of what you want to benchmark.\n",
       "\n",
       "```julia-repl\n",
       "julia> @benchmark sin(1)\n",
       "BenchmarkTools.Trial:\n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     13.610 ns (0.00% GC)\n",
       "  median time:      13.622 ns (0.00% GC)\n",
       "  mean time:        13.638 ns (0.00% GC)\n",
       "  maximum time:     21.084 ns (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     998\n",
       "```\n",
       "\n",
       "You can interpolate values into `@benchmark` expressions:\n",
       "\n",
       "```julia\n",
       "# rand(1000) is executed for each evaluation\n",
       "julia> @benchmark sum(rand(1000))\n",
       "BenchmarkTools.Trial:\n",
       "  memory estimate:  7.94 KiB\n",
       "  allocs estimate:  1\n",
       "  --------------\n",
       "  minimum time:     1.566 μs (0.00% GC)\n",
       "  median time:      2.135 μs (0.00% GC)\n",
       "  mean time:        3.071 μs (25.06% GC)\n",
       "  maximum time:     296.818 μs (95.91% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     10\n",
       "\n",
       "# rand(1000) is evaluated at definition time, and the resulting\n",
       "# value is interpolated into the benchmark expression\n",
       "julia> @benchmark sum($(rand(1000)))\n",
       "BenchmarkTools.Trial:\n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     101.627 ns (0.00% GC)\n",
       "  median time:      101.909 ns (0.00% GC)\n",
       "  mean time:        103.834 ns (0.00% GC)\n",
       "  maximum time:     276.033 ns (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     935\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  @benchmark <expr to benchmark> [setup=<setup expr>]\u001b[39m\n",
       "\n",
       "  Run benchmark on a given expression.\n",
       "\n",
       "\u001b[1m  Example\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  The simplest usage of this macro is to put it in front of what you want to\n",
       "  benchmark.\n",
       "\n",
       "\u001b[36m  julia> @benchmark sin(1)\u001b[39m\n",
       "\u001b[36m  BenchmarkTools.Trial:\u001b[39m\n",
       "\u001b[36m    memory estimate:  0 bytes\u001b[39m\n",
       "\u001b[36m    allocs estimate:  0\u001b[39m\n",
       "\u001b[36m    --------------\u001b[39m\n",
       "\u001b[36m    minimum time:     13.610 ns (0.00% GC)\u001b[39m\n",
       "\u001b[36m    median time:      13.622 ns (0.00% GC)\u001b[39m\n",
       "\u001b[36m    mean time:        13.638 ns (0.00% GC)\u001b[39m\n",
       "\u001b[36m    maximum time:     21.084 ns (0.00% GC)\u001b[39m\n",
       "\u001b[36m    --------------\u001b[39m\n",
       "\u001b[36m    samples:          10000\u001b[39m\n",
       "\u001b[36m    evals/sample:     998\u001b[39m\n",
       "\n",
       "  You can interpolate values into \u001b[36m@benchmark\u001b[39m expressions:\n",
       "\n",
       "\u001b[36m  # rand(1000) is executed for each evaluation\u001b[39m\n",
       "\u001b[36m  julia> @benchmark sum(rand(1000))\u001b[39m\n",
       "\u001b[36m  BenchmarkTools.Trial:\u001b[39m\n",
       "\u001b[36m    memory estimate:  7.94 KiB\u001b[39m\n",
       "\u001b[36m    allocs estimate:  1\u001b[39m\n",
       "\u001b[36m    --------------\u001b[39m\n",
       "\u001b[36m    minimum time:     1.566 μs (0.00% GC)\u001b[39m\n",
       "\u001b[36m    median time:      2.135 μs (0.00% GC)\u001b[39m\n",
       "\u001b[36m    mean time:        3.071 μs (25.06% GC)\u001b[39m\n",
       "\u001b[36m    maximum time:     296.818 μs (95.91% GC)\u001b[39m\n",
       "\u001b[36m    --------------\u001b[39m\n",
       "\u001b[36m    samples:          10000\u001b[39m\n",
       "\u001b[36m    evals/sample:     10\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # rand(1000) is evaluated at definition time, and the resulting\u001b[39m\n",
       "\u001b[36m  # value is interpolated into the benchmark expression\u001b[39m\n",
       "\u001b[36m  julia> @benchmark sum($(rand(1000)))\u001b[39m\n",
       "\u001b[36m  BenchmarkTools.Trial:\u001b[39m\n",
       "\u001b[36m    memory estimate:  0 bytes\u001b[39m\n",
       "\u001b[36m    allocs estimate:  0\u001b[39m\n",
       "\u001b[36m    --------------\u001b[39m\n",
       "\u001b[36m    minimum time:     101.627 ns (0.00% GC)\u001b[39m\n",
       "\u001b[36m    median time:      101.909 ns (0.00% GC)\u001b[39m\n",
       "\u001b[36m    mean time:        103.834 ns (0.00% GC)\u001b[39m\n",
       "\u001b[36m    maximum time:     276.033 ns (0.00% GC)\u001b[39m\n",
       "\u001b[36m    --------------\u001b[39m\n",
       "\u001b[36m    samples:          10000\u001b[39m\n",
       "\u001b[36m    evals/sample:     935\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?@benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 363 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m11.769 ms\u001b[22m\u001b[39m … \u001b[35m19.480 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m13.493 ms              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m13.770 ms\u001b[22m\u001b[39m ± \u001b[32m 1.376 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▄\u001b[39m▇\u001b[39m▆\u001b[39m▄\u001b[39m▅\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m█\u001b[34m▄\u001b[39m\u001b[39m▄\u001b[32m▆\u001b[39m\u001b[39m▂\u001b[39m▅\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▅\u001b[39m▄\u001b[39m▆\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▆\u001b[39m█\u001b[39m█\u001b[39m▅\u001b[39m█\u001b[39m▄\u001b[39m▆\u001b[39m▆\u001b[39m▄\u001b[39m▃\u001b[39m▅\u001b[39m▆\u001b[39m▃\u001b[39m▅\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m \u001b[39m▄\n",
       "  11.8 ms\u001b[90m         Histogram: frequency by time\u001b[39m          19 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m0 bytes\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m0\u001b[39m."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench = @benchmark seql_sum($x64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1my\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "propertynames(x, private=false)\n",
       "\\end{verbatim}\n",
       "Get a tuple or a vector of the properties (\\texttt{x.property}) of an object \\texttt{x}. This is typically the same as \\href{@ref}{\\texttt{fieldnames(typeof(x))}}, but types that overload \\href{@ref}{\\texttt{getproperty}} should generally overload \\texttt{propertynames} as well to get the properties of an instance of the type.\n",
       "\n",
       "\\texttt{propertynames(x)} may return only \"public\" property names that are part of the documented interface of \\texttt{x}.   If you want it to also return \"private\" fieldnames intended for internal use, pass \\texttt{true} for the optional second argument. REPL tab completion on \\texttt{x.} shows only the \\texttt{private=false} properties.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "propertynames(x, private=false)\n",
       "```\n",
       "\n",
       "Get a tuple or a vector of the properties (`x.property`) of an object `x`. This is typically the same as [`fieldnames(typeof(x))`](@ref), but types that overload [`getproperty`](@ref) should generally overload `propertynames` as well to get the properties of an instance of the type.\n",
       "\n",
       "`propertynames(x)` may return only \"public\" property names that are part of the documented interface of `x`.   If you want it to also return \"private\" fieldnames intended for internal use, pass `true` for the optional second argument. REPL tab completion on `x.` shows only the `private=false` properties.\n"
      ],
      "text/plain": [
       "\u001b[36m  propertynames(x, private=false)\u001b[39m\n",
       "\n",
       "  Get a tuple or a vector of the properties (\u001b[36mx.property\u001b[39m) of an object \u001b[36mx\u001b[39m. This\n",
       "  is typically the same as \u001b[36mfieldnames(typeof(x))\u001b[39m, but types that overload\n",
       "  \u001b[36mgetproperty\u001b[39m should generally overload \u001b[36mpropertynames\u001b[39m as well to get the\n",
       "  properties of an instance of the type.\n",
       "\n",
       "  \u001b[36mpropertynames(x)\u001b[39m may return only \"public\" property names that are part of\n",
       "  the documented interface of \u001b[36mx\u001b[39m. If you want it to also return \"private\"\n",
       "  fieldnames intended for internal use, pass \u001b[36mtrue\u001b[39m for the optional second\n",
       "  argument. REPL tab completion on \u001b[36mx.\u001b[39m shows only the \u001b[36mprivate=false\u001b[39m properties."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?propertynames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(:params, :times, :gctimes, :memory, :allocs)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propertynames(bench)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
